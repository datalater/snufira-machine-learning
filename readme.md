
---

### W1 :: 수학

+ 로그함수 : 계산하기 매우 어려운 연산에서 로그함수를 적용하면 수치 계산이 쉬워진다.
+ 편미분 : 변수가 여러 개일 때 한꺼번에 미분하지 못하므로 편미분을 한다.
+ 미분 : 최적화 값을 구하기 위해 사용한다.

+ $w^{T}x$ : 계수와 변수의 곱
+ 확률변수의 기대값을 계산할 때 각 값과 값의 확률을 곱해서 더하는데, 여기서 값의 확률이 머신러닝에서는 변수의 중요도가 된다.

### W1 :: 기계학습 개요

#### 선형회귀

+ model
+ 최소제곱법

systematic enumeration : 빼먹지 않고 중복하지 않고 데이터를 찾는다.

#### 최우추정법

+ Casino
+ 주사위 model
  + Fair model (1/6, ..., 1/6)
  + Loaded model (1/10, ..., 1/10, 1/2)
+ P(66666 | Fair) vs. P(66666 | Loaded)
  + Maximum Likelihood

#### Decision Tree

+ 무엇을 먼저 결정할지가 중요하다.
+ 정보 이론

---

#### 1. computability가 중요한 이유

+ 어떤 현상을 계산할 수 있는 model이 있느냐 없느냐가 중요하다
+ 알파고 : 바둑에 대한 model을 세웠다.
+ 감성에 대한 model? 아직 없다.
+ model이 없으면 계산할 수 없다.

#### 2. 사람과 같은 방식으로 model할 필요는 없다.

+ 알파고 : 사람과 다른 방식으로 바둑을 둔다.

---

#### 퍼셉트론

+ 차원을 높여서 데이터를 2개로 구분한다.

#### 딥러닝

+ 퍼셉트론을 네트워크로 연결해서 데이터들의 decision boundary를 결정한다.
+ 인공신경망은 decision boundary에 대한 함수가 무엇인지는 모르나 decision boundary를 구한다.
+ 작동은 되는데 작동의 이유는 알 수 없다.

#### 로지스틱 회귀

+ positive와 negative를 구분하고 싶다.
+ positive data에 대한 모델(M+)을 만든다.
+ negative data에 대한 모델(M-)을 만든다.


+ 회귀: 숫자값을 리턴하는 선형함수
+ 2가지 선형함수의 값을 서로 비교하는 것은 힘들다. ex. 여자 중에 100등과 남자 중에 100등 중 누가 더 뛰어난가? 알 수 없음.

#### K-means

+ 군집을 만든다.

#### K-nearest

+ 가까운 K

#### PCA

+ 변수가 수십 개일 때 전부 다 합쳐서 데이터의 분산이 maximize가 되는 가상의 축을 만들고,
+ 그 가상의 축의 직각으로

### W1 :: 알파고는 바둑을 어떻게 두는가, 랜덤 시뮬레이션

+ 랜덤 시뮬레이션 : pdf를 고려(guide)해서 랜덤하게 추출해야 한다.

+ 가이드란 domain knowledge를 기반으로 한다. 가령 하수의 데이터가 아니라 고수의 데이터를 참고해야 한다는 것이다. (그래서 빅데이터가 중요하다)

> **Note:** 우리나라의 펀드가 미국의 펀드보다 수익률이 낮은 것도 데이터의 양과 질이 부족하기 때문이다.

> **Note:** 거꾸로 우리나라는 전 세계적으로 전국민을 대상으로 의료보험을 하는 유일한 나라이다. 의료보험 데이터로 우리나라가 앞서 가야 한다고 말하는 주장은 설득력이 있는 주장이다.

+ 알파고가 바둑을 두는 메카니즘 : Reinforcement Learning와 simulation이다. 딥러닝이 아니다.

+ 사람은 몇 수 앞을 내다보고 바둑을 두지만, 알파고는 한 판을 다 둬본 후에 바둑을 둔다.
+ 이것을 random simulation이라 한다.
+ 누가 이기겠는가, 당연히 10수 앞을 내다보는 사람보다 한 판을 다 둬 본 기계가 이기게 된다.

### W1 :: 알파고가 바둑을 두는 방법

+ policy : 고수의 수
+ value network : 판세
+ policy network : 각 수에 10만 판을 뒀더니 승률이 어떻게 되는지를 구한 후 가장 많이 이기는 점에 수를 둔다.

---
